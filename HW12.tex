\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.2}

\pagestyle{fancy}
\rhead{\hmwkAuthorName}
\lhead{\hmwkClass: \hmwkTitle}


\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand{\proofname}{\textit{\textbf{ Proof.}}}

% \setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcommand{\hmwkTitle}{Assignment 12}
\newcommand{\hmwkClass}{Manifold Learning and Sparse Representation}
\newcommand{\hmwkAuthorName}{\textbf{ZHANG Yuan}, 1601111332 }
\date{}
%
% Title Page
%

\title{
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle
\section{Exercise 229.}
\begin{proof}
By the method of Lagrange multipliers,
$$
L = \Vert C \Vert_{*} + \frac{\alpha}{2} \Vert D - A \Vert_F^{2} + Y^{T}(A-AC).
$$
We assume that $A = U_r\Lambda_r V_r^{T}$, where $r$ is arbitrary, while $V_{-r}$ is a basis for the orthonormal to $V_r$ so that $I = V_rV_r^{T} + V_{-r}V_{-r}^{T}$. Thanks to Eq. (10.56), we know $C$ has a close-form solution, i.e., $V_rV_r^{T}$.  

Then, by the first order conditions, 
$$
V_rV_r^{T} + W = A^{T}Y =  V_r\Lambda_r U_r^{T}Y \Rightarrow W = V_r(\Lambda_r U_r^{T}Y - V_r^{T}) \Rightarrow V_{-r}^{T}W = 0,
$$
$$
Y(I-C^{T}) = \alpha(D-A) \Rightarrow D = A + \frac{1}{\alpha} YV_{-r}V_{-r}^{T}. 
$$
Since we also have $V_r^{T}W = 0$, $W = 0$. Then $U_r^{T}Y = \Lambda_r^{-1}V_r^{T}$, which implies $Y = U_r\Lambda_r^{-1} V_r^{T} + U_{-r}B$ for some $B$. By substitution to the second equation above, we have 
$$
D = A + \frac{1}{\alpha}(U_r\Lambda_r^{-1}V_r^{T} + U_{-r}B)V_{-r}V_{-r}^{T} = A + \frac{1}{\alpha} U_{-r}BV_{-r}V_{-r}^{T}.
$$

Thus, $\Vert D -A \Vert_F^2 = \frac{1}{\alpha^2}\Vert BV_{-r} \Vert_F^2$ which can be minimized when $\Lambda_{-r} = BV_{-r}$ is diagonal. This means that $D = [U_r,U_{-r}]\text{diag}(\Lambda_r,\Lambda_{-r})[V_r, V_{-r}]^T = [U_r,U_{-r}]\text{diag}(\Sigma_r,\Sigma_{-r})[V_r, V_{-r}]^T$ is a SVD for D. That is, $V_r$ and $U_r$ are top $r$ left and right singular vectors of $D$, respectively. Further, the loss function can be written as the following form
$$
\Vert C \Vert_{*} + \frac{\alpha}{2} \Vert D - A \Vert_F^{2} = r + \frac{1}{\alpha}\sum_{k>r}\sigma_k^2.
$$
\end{proof}

\section{Exercise 240.}
\begin{proof}

\begin{itemize}
    \item It is trivial to verify that those three conditions hold for any subset of domain $\Omega$ if itself contains any permutation of its elements, which can be proved by mathematical induction.
    \item $\bigcap_{i=1}^{m}\Omega_i \subset \Omega_i$ for all $i$ and then $\{f_i\}$ satisfies the EBD conditions on $\bigcap_{i=1}^{m}\Omega_i$, respectively. Therefore, by trivial verification, we can conclude then linear combinations of  $\{f_i\}$ satisfies the EBD conditions as well.
\end{itemize}
\end{proof}

% \section{Exercise 250.}
% \begin{proof}
% An easier proof than ref. [312] is offered as follows:

% Obviously, the objective function is strictly convex with regard to $Z$. Thus, there exists a unique minimizer and we only have to verify that Eq. (11.2) is a minimizer. 

% We know that $\hat{Z} = U\Sigma_{\lambda}U^{T}$ is a minimizer iff. 

% $$
% 0 \in \partial(\frac{1}{2}\Vert \phi(D) - \phi(D)\hat{Z} \Vert_F^2 + \lambda\Vert \hat{Z} \Vert_{*}) \Leftrightarrow \frac{1}{\lambda}K(I - \hat{Z}) \in   \partial \Vert  \hat{Z} \Vert_{*},
% $$
% where $K = \phi(D)\phi(D)^{T} = U \Sigma U^{T}$. This holds due to the following fact
% \begin{align*}
%     \frac{1}{\lambda}K(I - \hat{Z}) &=\frac{1}{\lambda} U \Sigma U^{T} ( I - U\Sigma_{\lambda}U^{T}) \\
%     &=  \frac{1}{\lambda} U \Sigma (I - \Sigma_{\lambda}) U^{T} \\
%     &= \frac{1}{\lambda} U \Sigma \text{diag}(\{\frac{\lambda}{\sigma_i}\vert \sigma_i > \lambda\},I) U^{T} \\
%     &= U \text{diag}(I,\{\frac{\sigma_i}{\lambda}\vert \sigma_i \leq \lambda \}) U^T \\
%     &= U_0U_0^{T} + U_1 W U_1^T,
% \end{align*}
% where $W = \text{diag}(\{\frac{\sigma_i}{\lambda}\vert \sigma_i \leq \lambda \}) = \text{diag}(\{\frac{\sigma_i}{\lambda}\vert \frac{\sigma_i}{\lambda} \leq 1 \})$ and $U_0$ ($U1$) consists of eigenvectors corresponding to those eigen-values that are larger (less) than $\lambda$. We can verify that $\Vert W \Vert_2 \leq 1$ and hence $\frac{1}{\lambda}K(I - \hat{Z}) \in   \partial \Vert  \hat{Z} \Vert_{*}$ thanks to Theorem 126.
% \end{proof}

\end{document}

