\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.2}

\pagestyle{fancy}
\rhead{\hmwkAuthorName}
\lhead{\hmwkClass: \hmwkTitle}


\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand{\proofname}{\textit{\textbf{ Proof.}}}

% \setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcommand{\hmwkTitle}{Assignment 1}
\newcommand{\hmwkDueDate}{February 12, 2014}
\newcommand{\hmwkClass}{Manifold Learning and Sparse Representation}
\newcommand{\hmwkAuthorName}{\textbf{ZHANG Yuan}, 1601111332 }
\date{}


%
% Title Page
%

\title{
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

%\pagebreak
\begin{section}{Exercise 1.}
The trace of A is 

$$
tr(A) =tr
\begin{pmatrix}
1 & 2 & 3\\
2 & 5 & 5\\
3 & 6 & 9
\end{pmatrix} = 15.
$$

\end{section}

\begin{section}{Exercise 2.}
The eigenvalue decomposition of A is 

\begin{align*}
A &=
\begin{pmatrix}
1 & 2 & 3\\
2 & 5 & 5\\
3 & 6 & 9
\end{pmatrix}
\\ 
&=
\begin{pmatrix}
  -0.2615  &  0.1778 &  -0.9487\\
  -0.5623 &  -0.8269  &  0.0000\\
  -0.7845 &   0.5335  &  0.3162\\
\end{pmatrix} 
\cdot
diag(14.3007,0.6993,0)
\cdot
\begin{pmatrix}
  -0.2615  &  0.1778 &  -0.9487\\
  -0.5623 &  -0.8269  &  0.0000\\
  -0.7845 &   0.5335  &  0.3162\\
\end{pmatrix}^T.
\end{align*}
\end{section}

\begin{section}{Exercise 3.}
The full SVD of A is
\begin{align*}
A &=
\begin{pmatrix}
1 & 2 & 3\\
2 & 5 & 5\\
3 & 6 & 9\\
10 & 11 & 12
\end{pmatrix}
\\ 
&=
\begin{pmatrix}
   -0.1409  &  0.8247  &  0.5477  & -0.0037 \\
   -0.3439  &  0.4263 &  -0.7276  &  0.4131 \\
   -0.5470  &  0.0278 &  -0.1880  & -0.8153 \\
   -0.7501 &  -0.3706  &  0.3679 &   0.4058 
\end{pmatrix} 
\cdot
\begin{pmatrix}
   25.4624       &  0    &     0 \\
         0  &  1.2907    &     0\\
         0   &      0   &     0    \\
         0    &     0      &   0\\
\end{pmatrix}
\cdot
\begin{pmatrix}
   -0.5045  & -0.7608 &  -0.4082 \\
   -0.5745  & -0.0571  &  0.8165\\
   -0.6445  &  0.6465 &  -0.4082\\
\end{pmatrix}^T.
\end{align*}
The skinny SVD of A is
\begin{align*}
A &=
\begin{pmatrix}
1 & 2 & 3\\
2 & 5 & 5\\
3 & 6 & 9\\
10 & 11 & 12
\end{pmatrix}
\\ 
&=
\begin{pmatrix}
   -0.1409  &  0.8247  &  0.5477  \\
   -0.3439  &  0.4263 &  -0.7276  \\
   -0.5470  &  0.0278 &  -0.1880  \\
   -0.7501 &  -0.3706  &  0.3679
\end{pmatrix} 
\cdot
\begin{pmatrix}
   25.4624       &  0    &     0 \\
         0  &  1.2907    &     0\\
         0   &      0   &     0    \\
\end{pmatrix}
\cdot
\begin{pmatrix}
   -0.5045  & -0.7608 &  -0.4082 \\
   -0.5745  & -0.0571  &  0.8165\\
   -0.6445  &  0.6465 &  -0.4082\\
\end{pmatrix}^T.
\end{align*}
\end{section}
\begin{section}{Exercise 4.}
Thanks to $P_X = X(X^TX)^{-1}X^T$, we can obtain the projection matrix associated with $X$ as
$$
P_X = \begin{pmatrix}
    0.4794 &  -0.1850 &  -0.2398  &  0.3973\\
   -0.1850  &  0.9343 &  -0.0852  &  0.1412\\
   -0.2398 &  -0.0852  &  0.8896 &   0.1830\\
    0.3973 &  0.1412  &  0.1830  &  0.6968
\end{pmatrix} .
$$
\end{section}

\begin{section}{Exercise 5.}
\begin{proof}
We denote $AXB$ by $C$. We have 
$$
C_{i,j} = \sum_{k=1}^{p}\sum_{w=1}^{n}A_{i,k}X_{k,w}B_{w,j}.
$$
Thus, we can obtain
$$
LHS = \begin{pmatrix}
C_{:,1} \\
\vdots \\
C_{:,q}\\
\end{pmatrix}
=
\begin{pmatrix}
\sum_{k=1}^{p}\sum_{w=1}^{n}A_{:,k}X_{k,w}B_{w,1} \\
\vdots \\
\sum_{k=1}^{p}\sum_{w=1}^{n}A_{:,k}X_{k,w}B_{w,q}\\
\end{pmatrix}
=
\begin{pmatrix}
\sum_{w=1}^{n}B_{w,1}AX_{:,w} \\
\vdots \\
\sum_{w=1}^{n}B_{w,q}AX_{:,w}\\
\end{pmatrix},
$$
where we make use of the fact that $\sum_{k=1}^{p}A_{:,k}X_{k,w} = AX_{:,w} $ and $B_{w,1},...,B_{w,q}$ are commutative constants.

Also, by definition of the Kronecker product,
$$
B^T\otimes A = 
\begin{pmatrix}
B_{1,1}A &  \cdots & B_{n,1}A \\
\vdots & \ddots & \vdots \\
B_{1,q}A & \cdots & B_{n,q}A
\end{pmatrix}.
$$
Then, we conclude by
$$
RHS = B^T\otimes A\cdot vec(X) =
\begin{pmatrix}
B_{1,1}A &  \cdots & B_{n,1}A \\
\vdots & \ddots & \vdots \\
B_{1,q}A & \cdots & B_{n,q}A
\end{pmatrix}\cdot
\begin{pmatrix}
X_{:,1}\\
\vdots \\
X_{:,n}
\end{pmatrix}
=
\begin{pmatrix}
\sum_{w=1}^{n} B_{w,1}AX_{:,w} \\
\vdots \\
\sum_{w=1}^{n} B_{w,q}AX_{:,w}\\
\end{pmatrix} 
= LHS.
$$
\end{proof}
\end{section}
\begin{section}{Exercise 8.}
\begin{proof}
We first note that
\begin{equation}
\begin{aligned}
&A^{-1}U(C^{-1}+V^{T}A^{-1}U)^{-1}V^{T}A^{-1} \\
=&A^{-1}[AV^{-T}(C^{-1}+V^{T}A^{-1}U)U^{-1}]^{-1} \\
=&A^{-1}[(AV^{-T}C^{-1}+U)U^{-1}]^{-1}\\
=&A^{-1}[(A+UCV^{T})V^{-T}C^{-1}U^{-1}]^{-1}\\
=&A^{-1}UCV^{T}(A+UCV^{T})^{-1}
\end{aligned}  
\end{equation}
By substituting Eq.(1), we can verify that
$$
\begin{aligned}
&[A^{-1} - A^{-1}U(C^{-1}+V^{T}A^{-1}U)^{-1}V^{T}A^{-1}]\cdot(A+UCV^{T}) \\
=&A^{-1}(A+UCV^{T}) - A^{-1}UCV^{T}(A+UCV^{T})^{-1}(A+UCV^{T})\\
=&I + A^{-1}UCV^{T}-A^{-1}UCV^{T} = I
\end{aligned} 
$$
Thus, $(A+UCV^{T})^{-1} = A^{-1} - A^{-1}U(C^{-1}+V^{T}A^{-1}U)^{-1}V^{T}A^{-1}$.
\end{proof}
\end{section}

\begin{section}{Exercise 21.}
The singular values of A denoted by $\sigma_1$,$\sigma_2$ and $\sigma_3$ are $25.4624$, $1.2907$ and $0$, respectively.
\begin{itemize}
    \item The 1-norm is $\max\{1+4+7+10,2+5+8+11,3+6+9+12\}=30$.
    \item The 2-norm is $\sigma_1(A) \approx 25.4624$.
    \item The Frobenious norm is $\sqrt{\sum_{i=1}^{3}\sigma_i^2}\approx\sqrt{25.4624^2 +1.2907^2+0^2} \approx25.4951 $.
    \item The $\inf$-norm is $\max\{1+2+3,4+5+6,7+8+9,10+11+12\}=33$.
    \item The nuclear norm is $\sum_{i=1}^{3}\sigma_i \approx 25.4624 +1.2907+0 = 26.7531$.
    \item The (2,1)-norm is $\sum_{i=1}^{n}\Vert A_i \Vert_2 \approx 43.9445$.
\end{itemize}

\end{section}

\end{document}